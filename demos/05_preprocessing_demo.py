"""
Demo: Data Preprocessing Operations
===================================
This demo showcases preprocessing functions in dskit.
"""

from dskit import auto_encode, auto_scale, train_test_auto
import pandas as pd
import numpy as np

def create_sample_data():
    """Create sample dataset for preprocessing"""
    np.random.seed(42)
    return pd.DataFrame({
        'age': np.random.randint(18, 70, 200),
        'salary': np.random.randint(30000, 150000, 200),
        'experience': np.random.randint(0, 30, 200),
        'department': np.random.choice(['IT', 'HR', 'Sales', 'Marketing'], 200),
        'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], 200),
        'location': np.random.choice(['NYC', 'SF', 'LA', 'Chicago', 'Boston'], 200),
        'performance': np.random.choice(['Low', 'Medium', 'High'], 200)
    })


def demo_auto_encode():
    """Demo 1: Automatic encoding of categorical variables"""
    print("=" * 60)
    print("DEMO 1: Automatic Categorical Encoding")
    print("=" * 60)
    
    df = create_sample_data()
    
    print("\nğŸ“Š Original data shape:", df.shape)
    print("ğŸ“Š Original columns:", list(df.columns))
    print("\nğŸ“Š Categorical columns:")
    cat_cols = df.select_dtypes(include='object').columns.tolist()
    for col in cat_cols:
        print(f"  - {col}: {df[col].nunique()} unique values")
    
    print("\nğŸ”§ Applying automatic encoding...")
    print("   (Uses One-Hot for low cardinality, Label for high cardinality)")
    
    df_encoded = auto_encode(df, max_unique_for_onehot=10)
    
    print("\nâœ“ Encoded data shape:", df_encoded.shape)
    print("âœ“ New columns:", list(df_encoded.columns))
    print(f"âœ“ Added {df_encoded.shape[1] - df.shape[1]} new columns")
    
    print("\nğŸ“Š Sample of encoded data:")
    print(df_encoded.head())


def demo_auto_scale():
    """Demo 2: Automatic feature scaling"""
    print("\n" + "=" * 60)
    print("DEMO 2: Automatic Feature Scaling")
    print("=" * 60)
    
    df = create_sample_data()
    df_encoded = auto_encode(df)
    
    print("\nğŸ“Š Original numeric ranges:")
    numeric_cols = df_encoded.select_dtypes(include=[np.number]).columns[:3]
    for col in numeric_cols:
        print(f"  {col}: [{df_encoded[col].min():.2f}, {df_encoded[col].max():.2f}]")
    
    # Standard scaling
    print("\nğŸ”§ Applying Standard Scaling...")
    df_standard = auto_scale(df_encoded, method='standard')
    
    print("\nâœ“ After Standard Scaling:")
    for col in numeric_cols:
        print(f"  {col}: mean={df_standard[col].mean():.4f}, std={df_standard[col].std():.4f}")
    
    # MinMax scaling
    print("\nğŸ”§ Applying MinMax Scaling...")
    df_minmax = auto_scale(df_encoded, method='minmax')
    
    print("\nâœ“ After MinMax Scaling:")
    for col in numeric_cols:
        print(f"  {col}: [{df_minmax[col].min():.2f}, {df_minmax[col].max():.2f}]")
    
    # Robust scaling
    print("\nğŸ”§ Applying Robust Scaling...")
    df_robust = auto_scale(df_encoded, method='robust')
    
    print("\nâœ“ After Robust Scaling:")
    for col in numeric_cols:
        print(f"  {col}: median={df_robust[col].median():.4f}")


def demo_train_test_split():
    """Demo 3: Automatic train-test splitting"""
    print("\n" + "=" * 60)
    print("DEMO 3: Train-Test Split")
    print("=" * 60)
    
    df = create_sample_data()
    df_encoded = auto_encode(df)
    df_scaled = auto_scale(df_encoded)
    
    print("\nğŸ“Š Full dataset shape:", df_scaled.shape)
    
    print("\nğŸ”§ Splitting data (80-20 split)...")
    X_train, X_test, y_train, y_test = train_test_auto(
        df_scaled, 
        target='performance', 
        test_size=0.2, 
        random_state=42
    )
    
    print("\nâœ“ Split completed:")
    print(f"  Training set: {X_train.shape[0]} samples, {X_train.shape[1]} features")
    print(f"  Test set: {X_test.shape[0]} samples, {X_test.shape[1]} features")
    print(f"  Target distribution in train: {y_train.value_counts().to_dict()}")
    print(f"  Target distribution in test: {y_test.value_counts().to_dict()}")


def demo_complete_pipeline():
    """Demo 4: Complete preprocessing pipeline"""
    print("\n" + "=" * 60)
    print("DEMO 4: Complete Preprocessing Pipeline")
    print("=" * 60)
    
    print("\nğŸ“Š Starting with raw data...")
    df = create_sample_data()
    print(f"  Shape: {df.shape}")
    
    print("\nğŸ”§ Step 1: Encoding categorical variables...")
    df_encoded = auto_encode(df, max_unique_for_onehot=10)
    print(f"  âœ“ Shape after encoding: {df_encoded.shape}")
    
    print("\nğŸ”§ Step 2: Scaling features...")
    df_scaled = auto_scale(df_encoded, method='standard')
    print(f"  âœ“ Shape after scaling: {df_scaled.shape}")
    
    print("\nğŸ”§ Step 3: Splitting into train/test...")
    X_train, X_test, y_train, y_test = train_test_auto(
        df_scaled, 
        target='performance', 
        test_size=0.2, 
        random_state=42
    )
    print(f"  âœ“ Train set: {X_train.shape}")
    print(f"  âœ“ Test set: {X_test.shape}")
    
    print("\nâœ… Preprocessing pipeline completed!")
    print("   Data is now ready for modeling")


if __name__ == "__main__":
    print("\n" + "âš™ï¸" * 30)
    print("PREPROCESSING OPERATIONS DEMO".center(60))
    print("âš™ï¸" * 30 + "\n")
    
    demo_auto_encode()
    demo_auto_scale()
    demo_train_test_split()
    demo_complete_pipeline()
    
    print("\n" + "âœ…" * 30)
    print("ALL DEMOS COMPLETED".center(60))
    print("âœ…" * 30 + "\n")
